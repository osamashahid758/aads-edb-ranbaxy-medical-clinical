"""
EDB Logging Utilities
Business Use Case: ranbaxy-medical-data
Generated by: EDB Project Onboarding Framework

This module provides standardized logging utilities for EDB Glue jobs
following enterprise standards for:
- Structured logging format
- Data quality metrics
- Performance monitoring  
- Error tracking and alerting
"""

import json
import logging
import sys
from datetime import datetime
from typing import Dict, Any, Optional
import boto3
from botocore.exceptions import ClientError


class EDBStructuredFormatter(logging.Formatter):
    """Custom formatter for EDB structured logging."""
    
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # Add custom fields if they exist
        if hasattr(record, 'job_name'):
            log_entry['job_name'] = record.job_name
        if hasattr(record, 'environment'):
            log_entry['environment'] = record.environment
        if hasattr(record, 'business_usecase'):
            log_entry['business_usecase'] = record.business_usecase
        if hasattr(record, 'batch_id'):
            log_entry['batch_id'] = record.batch_id
            
        return json.dumps(log_entry)


def setup_edb_logger(job_name: str, environment: str, business_usecase: str = "ranbaxy-medical-data") -> logging.Logger:
    """
    Set up standardized EDB logger with structured formatting.
    
    Args:
        job_name: Name of the Glue job
        environment: Environment (dev/qa/prod)
        business_usecase: Business use case identifier
        
    Returns:
        Configured logger instance
    """
    logger = logging.getLogger(job_name)
    logger.setLevel(logging.INFO)
    
    # Remove existing handlers to avoid duplicates
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Create console handler with custom formatter
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    formatter = EDBStructuredFormatter()
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    
    # Add custom attributes for all log messages
    logger = logging.LoggerAdapter(logger, {
        'job_name': job_name,
        'environment': environment,
        'business_usecase': business_usecase
    })
    
    return logger


def get_logger(glue_ctx):
    """Legacy function for compatibility with existing Glue jobs."""
    return glue_ctx.get_logger()


def log_data_quality_metrics(logger: logging.Logger, metrics: Dict[str, Any]) -> None:
    """
    Log data quality metrics in standardized format.
    
    Args:
        logger: Logger instance
        metrics: Dictionary containing data quality metrics
    """
    standardized_metrics = {
        "metric_type": "data_quality",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "job_name": metrics.get("job_name", "unknown"),
        "environment": metrics.get("environment", "unknown"),
        "business_usecase": metrics.get("business_usecase", "ranbaxy-medical-data"),
        "total_records": metrics.get("total_records", 0),
        "processed_records": metrics.get("processed_records", 0),
        "error_records": metrics.get("error_records", 0),
        "null_counts": metrics.get("null_counts", {}),
        "duplicate_counts": metrics.get("duplicate_counts", 0),
        "data_freshness_hours": metrics.get("data_freshness_hours", 0),
        "processing_duration_seconds": metrics.get("processing_duration_seconds", 0)
    }
    
    logger.info(f"DATA_QUALITY_METRICS: {json.dumps(standardized_metrics)}")
    
    # Send metrics to CloudWatch if in production
    environment = metrics.get("environment", "dev")
    if environment in ["qa", "prod"]:
        try:
            send_cloudwatch_metrics(standardized_metrics)
        except Exception as e:
            logger.warning(f"Failed to send CloudWatch metrics: {str(e)}")


def send_cloudwatch_metrics(metrics: Dict[str, Any]) -> None:
    """
    Send data quality metrics to CloudWatch.
    
    Args:
        metrics: Metrics dictionary
    """
    try:
        cloudwatch = boto3.client('cloudwatch')
        
        namespace = f"EDB/{metrics['business_usecase'].upper()}"
        
        metric_data = [
            {
                'MetricName': 'TotalRecords',
                'Value': metrics['total_records'],
                'Unit': 'Count',
                'Dimensions': [
                    {
                        'Name': 'JobName',
                        'Value': metrics['job_name']
                    },
                    {
                        'Name': 'Environment', 
                        'Value': metrics['environment']
                    }
                ]
            },
            {
                'MetricName': 'ProcessedRecords',
                'Value': metrics['processed_records'],
                'Unit': 'Count',
                'Dimensions': [
                    {
                        'Name': 'JobName',
                        'Value': metrics['job_name']
                    },
                    {
                        'Name': 'Environment',
                        'Value': metrics['environment']
                    }
                ]
            },
            {
                'MetricName': 'ErrorRecords',
                'Value': metrics['error_records'],
                'Unit': 'Count',
                'Dimensions': [
                    {
                        'Name': 'JobName',
                        'Value': metrics['job_name']
                    },
                    {
                        'Name': 'Environment',
                        'Value': metrics['environment']
                    }
                ]
            },
            {
                'MetricName': 'ProcessingDuration',
                'Value': metrics['processing_duration_seconds'],
                'Unit': 'Seconds',
                'Dimensions': [
                    {
                        'Name': 'JobName',
                        'Value': metrics['job_name']
                    },
                    {
                        'Name': 'Environment',
                        'Value': metrics['environment']
                    }
                ]
            }
        ]
        
        cloudwatch.put_metric_data(
            Namespace=namespace,
            MetricData=metric_data
        )
        
    except ClientError as e:
        print(f"Failed to send CloudWatch metrics: {e}")


def log_performance_metrics(logger: logging.Logger, operation: str, duration_seconds: float, record_count: int = 0) -> None:
    """
    Log performance metrics for specific operations.
    
    Args:
        logger: Logger instance
        operation: Name of the operation (e.g., 'read_s3', 'transform_data', 'write_s3')
        duration_seconds: Duration of the operation in seconds
        record_count: Number of records processed
    """
    performance_metrics = {
        "metric_type": "performance",
        "timestamp": datetime.utcnow().isoformat() + "Z", 
        "operation": operation,
        "duration_seconds": duration_seconds,
        "record_count": record_count,
        "records_per_second": record_count / duration_seconds if duration_seconds > 0 else 0
    }
    
    logger.info(f"PERFORMANCE_METRICS: {json.dumps(performance_metrics)}")


def log_error_with_context(logger: logging.Logger, error: Exception, context: Dict[str, Any]) -> None:
    """
    Log errors with additional context for debugging.
    
    Args:
        logger: Logger instance
        error: Exception that occurred
        context: Additional context information
    """
    error_info = {
        "error_type": "job_error",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "error_class": error.__class__.__name__,
        "error_message": str(error),
        "context": context
    }
    
    logger.error(f"ERROR_WITH_CONTEXT: {json.dumps(error_info)}")


def send_sns_alert(topic_arn: str, subject: str, message: str, job_name: str, environment: str) -> None:
    """
    Send SNS alert for critical errors or issues.
    
    Args:
        topic_arn: SNS topic ARN
        subject: Alert subject
        message: Alert message
        job_name: Name of the job
        environment: Environment
    """
    try:
        sns = boto3.client('sns')
        
        alert_message = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "job_name": job_name,
            "environment": environment,
            "business_usecase": "ranbaxy-medical-data",
            "alert_type": "job_failure",
            "subject": subject,
            "message": message
        }
        
        sns.publish(
            TopicArn=topic_arn,
            Subject=f"EDB Alert: {subject}",
            Message=json.dumps(alert_message, indent=2)
        )
        
    except ClientError as e:
        print(f"Failed to send SNS alert: {e}")


def validate_data_quality(df, job_name: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    Perform basic data quality validation on a DataFrame.
    
    Args:
        df: Spark DataFrame to validate
        job_name: Name of the job
        logger: Logger instance
        
    Returns:
        Dictionary containing validation results
    """
    try:
        total_records = df.count()
        column_count = len(df.columns)
        
        # Check for null values in each column
        null_counts = {}
        for col in df.columns:
            null_count = df.filter(df[col].isNull()).count()
            if null_count > 0:
                null_counts[col] = null_count
        
        # Check for duplicate records (if 'id' column exists)
        duplicate_count = 0
        if 'id' in df.columns:
            duplicate_count = df.count() - df.dropDuplicates(['id']).count()
        
        validation_results = {
            "validation_timestamp": datetime.utcnow().isoformat() + "Z",
            "job_name": job_name,
            "total_records": total_records,
            "column_count": column_count,
            "null_counts": null_counts,
            "duplicate_count": duplicate_count,
            "has_data_quality_issues": len(null_counts) > 0 or duplicate_count > 0
        }
        
        logger.info(f"DATA_VALIDATION_RESULTS: {json.dumps(validation_results)}")
        
        return validation_results
        
    except Exception as e:
        logger.error(f"Data quality validation failed: {str(e)}")
        return {
            "validation_timestamp": datetime.utcnow().isoformat() + "Z",
            "job_name": job_name,
            "validation_error": str(e),
            "has_data_quality_issues": True
        }