#!/usr/bin/env python3
"""
Glue ETL Job for ranbaxy medical data
Generated by EDB Project Onboarding Wizard

Description: need to transfer ranbaxy files to s3 in edb
Source: S3 - ranbaxy medical data-source
Target: S3 - ranbaxy medical data-target
Generated on: 2025-10-03T19:23:56.032136
"""

import sys
import json
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

# AWS Glue imports
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# PySpark imports
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def main():
    """Main ETL job execution function"""
    
    # Get job arguments
    args = getResolvedOptions(sys.argv, [
        "JOB_NAME",
        "config_s3_uri"
    ])
    
    # Initialize Glue context
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args["JOB_NAME"], args)
    
    logger.info(f"üöÄ Starting job: {args['JOB_NAME']}")
    logger.info(f"üìã Config URI: {args['config_s3_uri']}")
    
    try:
        # Load configuration
        config = load_config(spark, args["config_s3_uri"])
        logger.info("‚úÖ Configuration loaded successfully")
        
        # Validate configuration
        validate_config(config)
        logger.info("‚úÖ Configuration validation passed")
        
        # Read source data
        logger.info("üì• Reading source data...")
        source_df = read_source_data(glueContext, config["source"])
        
        if source_df is None:
            raise ValueError("Failed to read source data")
            
        initial_count = source_df.count()
        logger.info(f"üìä Source data loaded: {initial_count:,} records")
        
        # Apply transformations
        logger.info("üîÑ Applying transformations...")
        transformed_df = apply_transformations(spark, source_df, [])
        
        final_count = transformed_df.count()
        logger.info(f"üìä Transformed data: {final_count:,} records")
        
        # Data quality checks
        logger.info("üîç Performing data quality checks...")
        quality_results = perform_quality_checks(transformed_df, config.get("quality_rules", []))
        log_quality_results(quality_results)
        
        # Write target data
        logger.info("üì§ Writing target data...")
        write_target_data(glueContext, transformed_df, config["target"])
        
        # Log job completion metrics
        logger.info("üìà Job metrics:")
        logger.info(f"   - Records processed: {initial_count:,}")
        logger.info(f"   - Records output: {final_count:,}")
        logger.info(f"   - Data quality score: {quality_results.get('overall_score', 'N/A')}")
        logger.info("‚úÖ ETL job completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Job failed: {str(e)}")
        raise
    finally:
        job.commit()


def load_config(spark: SparkSession, config_s3_uri: str) -> Dict[str, Any]:
    """Load configuration from S3"""
    try:
        # For S3 URIs, we need to handle them specially in Glue
        if config_s3_uri.startswith('s3://'):
            # Use Spark to read the config file
            bucket_and_key = config_s3_uri.replace('s3://', '').split('/', 1)
            bucket = bucket_and_key[0]
            key = bucket_and_key[1] if len(bucket_and_key) > 1 else ''
            
            # Read the JSON config file
            config_df = spark.read.option("multiline", "true").json(config_s3_uri)
            config_json = config_df.toJSON().first()
            config = json.loads(config_json)
        else:
            # Local file for testing
            with open(config_s3_uri, 'r') as f:
                config = json.load(f)
                
        return config
        
    except Exception as e:
        logger.error(f"Failed to load config from {config_s3_uri}: {str(e)}")
        # Return default configuration
        return {
            "source": {
                "type": "S3",
                "name": "ranbaxy medical data-source",
                "connection_details": {}
            },
            "target": {
                "type": "S3",
                "name": "ranbaxy medical data-target",
                "connection_details": {},
                "output_format": "Parquet"
            },
            "transformations": []  # Transformations step removed
        }


def validate_config(config: Dict[str, Any]) -> None:
    """Validate configuration parameters"""
    required_keys = ["source", "target"]
    for key in required_keys:
        if key not in config:
            raise ValueError(f"Missing required configuration key: {key}")
    
    # Validate source config
    source = config["source"]
    if "type" not in source or "name" not in source:
        raise ValueError("Source configuration must include 'type' and 'name'")
    
    # Validate target config
    target = config["target"]
    if "type" not in target or "name" not in target:
        raise ValueError("Target configuration must include 'type' and 'name'")


def read_source_data(glueContext: GlueContext, source_config: Dict[str, Any]) -> DataFrame:
    """Read data from the configured source"""
    source_type = source_config.get("type")
    source_name = source_config.get("name", "Unknown")
    connection_details = source_config.get("connection_details", {})
    
    logger.info(f"üì• Reading from {source_type} source: {source_name}")
    
    try:
        if source_type == "S3":
            return read_s3_source(glueContext, connection_details)
        elif source_type == "Database":
            return read_database_source(glueContext, connection_details)
        elif source_type == "API":
            return read_api_source(glueContext, connection_details)
        else:
            raise ValueError(f"Unsupported source type: {source_type}")
            
    except Exception as e:
        logger.error(f"‚ùå Failed to read source data: {str(e)}")
        raise


def read_s3_source(glueContext: GlueContext, connection_details: Dict[str, Any]) -> DataFrame:
    """Read data from S3"""
    bucket = connection_details.get("bucket", "")
    prefix = connection_details.get("prefix", "")
    format_type = connection_details.get("format", "json").lower()
    
    s3_path = f"s3://{bucket}/{prefix}" if prefix else f"s3://{bucket}/"
    logger.info(f"üìÅ Reading {format_type.upper()} files from: {s3_path}")
    
    # Create dynamic frame based on format
    if format_type == "json":
        dyf = glueContext.create_dynamic_frame.from_options(
            format_options={"multiline": True},
            connection_type="s3",
            format="json",
            connection_options={
                "paths": [s3_path],
                "recurse": True
            }
        )
    elif format_type == "parquet":
        dyf = glueContext.create_dynamic_frame.from_options(
            connection_type="s3",
            format="parquet",
            connection_options={
                "paths": [s3_path],
                "recurse": True
            }
        )
    elif format_type == "csv":
        dyf = glueContext.create_dynamic_frame.from_options(
            format_options={
                "withHeader": connection_details.get("header", True),
                "separator": connection_details.get("delimiter", ",")
            },
            connection_type="s3",
            format="csv",
            connection_options={
                "paths": [s3_path],
                "recurse": True
            }
        )
    else:
        raise ValueError(f"Unsupported S3 format: {format_type}")
    
    return dyf.toDF()


def read_database_source(glueContext: GlueContext, connection_details: Dict[str, Any]) -> DataFrame:
    """Read data from database"""
    connection_name = connection_details.get("connection_name")
    table_name = connection_details.get("table_name")
    
    if connection_name:
        # Use Glue Data Catalog connection
        dyf = glueContext.create_dynamic_frame.from_catalog(
            database=connection_details.get("database"),
            table_name=table_name,
            connection_name=connection_name
        )
    else:
        # Direct JDBC connection
        connection_options = {
            "url": connection_details.get("jdbc_url"),
            "dbtable": table_name,
            "user": connection_details.get("username", ""),
            "password": connection_details.get("password", "")
        }
        
        dyf = glueContext.create_dynamic_frame.from_options(
            connection_type="postgresql",  # Default to PostgreSQL
            connection_options=connection_options
        )
    
    return dyf.toDF()


def read_api_source(glueContext: GlueContext, connection_details: Dict[str, Any]) -> DataFrame:
    """Read data from API (placeholder - requires custom implementation)"""
    # This is a placeholder - actual API reading would need custom logic
    logger.warning("‚ö†Ô∏è API source reading not fully implemented - using mock data")
    
    # Create a simple mock dataframe for demonstration
    spark = glueContext.spark_session
    mock_data = [{"id": 1, "name": "Mock Data", "timestamp": datetime.now().isoformat()}]
    return spark.createDataFrame(mock_data)


def apply_transformations(spark: SparkSession, df: DataFrame, transformations: List[Dict[str, Any]]) -> DataFrame:
    """Apply configured transformations to the data"""
    current_df = df
    
    if not transformations:
        logger.info("‚ÑπÔ∏è No transformations configured")
        return current_df
    
    for i, transform in enumerate(transformations):
        if not transform.get("enabled", True):
            logger.info(f"‚è≠Ô∏è Skipping disabled transformation: {transform.get('name', f'Transform {i+1}')}")
            continue
            
        transform_type = transform.get("type")
        transform_name = transform.get("name", f"Transform {i+1}")
        config = transform.get("config", {})
        
        logger.info(f"üîÑ Applying transformation {i+1}: {transform_name} ({transform_type})")
        
        try:
            if transform_type == "map_columns":
                current_df = apply_column_mapping(current_df, config)
            elif transform_type == "filter_rows":
                current_df = apply_row_filtering(current_df, config)
            elif transform_type == "cast_types":
                current_df = apply_type_casting(current_df, config)
            elif transform_type == "uuid_cast":
                current_df = apply_uuid_casting(current_df, config)
            elif transform_type == "aggregate":
                current_df = apply_aggregation(current_df, config)
            elif transform_type == "custom_sql":
                current_df = apply_custom_sql(spark, current_df, config)
            else:
                logger.warning(f"‚ö†Ô∏è Unknown transformation type: {transform_type}")
                
        except Exception as e:
            logger.error(f"‚ùå Transformation {transform_name} failed: {str(e)}")
            if config.get("fail_on_error", True):
                raise
            else:
                logger.warning(f"‚ö†Ô∏è Continuing despite transformation failure")
    
    return current_df


def apply_column_mapping(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply column mapping/renaming"""
    mappings = config.get("mappings", {})
    result_df = df
    
    for old_col, new_col in mappings.items():
        if old_col in result_df.columns:
            result_df = result_df.withColumnRenamed(old_col, new_col)
            logger.info(f"   üìù Renamed column: {old_col} -> {new_col}")
        else:
            logger.warning(f"   ‚ö†Ô∏è Column not found for renaming: {old_col}")
    
    return result_df


def apply_row_filtering(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply row filtering based on conditions"""
    condition = config.get("condition", "")
    
    if not condition:
        logger.warning("   ‚ö†Ô∏è No filter condition specified")
        return df
    
    try:
        result_df = df.filter(condition)
        filtered_count = result_df.count()
        original_count = df.count()
        logger.info(f"   üîç Filtered rows: {original_count:,} -> {filtered_count:,}")
        return result_df
    except Exception as e:
        logger.error(f"   ‚ùå Filter condition failed: {condition}")
        raise


def apply_type_casting(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply data type casting"""
    type_mappings = config.get("type_mappings", {})
    result_df = df
    
    for column, target_type in type_mappings.items():
        if column in result_df.columns:
            try:
                if target_type.lower() == "string":
                    result_df = result_df.withColumn(column, col(column).cast(StringType()))
                elif target_type.lower() == "integer":
                    result_df = result_df.withColumn(column, col(column).cast(IntegerType()))
                elif target_type.lower() == "double":
                    result_df = result_df.withColumn(column, col(column).cast(DoubleType()))
                elif target_type.lower() == "timestamp":
                    result_df = result_df.withColumn(column, col(column).cast(TimestampType()))
                else:
                    logger.warning(f"   ‚ö†Ô∏è Unknown type for casting: {target_type}")
                    continue
                    
                logger.info(f"   üîÑ Cast column {column} to {target_type}")
            except Exception as e:
                logger.error(f"   ‚ùå Failed to cast {column} to {target_type}: {str(e)}")
                if config.get("fail_on_error", True):
                    raise
        else:
            logger.warning(f"   ‚ö†Ô∏è Column not found for type casting: {column}")
    
    return result_df


def apply_uuid_casting(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply UUID casting with proper error handling"""
    uuid_columns = config.get("uuid_columns", [])
    null_handling = config.get("null_handling", "skip")  # skip, default, error
    
    result_df = df
    
    for column in uuid_columns:
        if column not in result_df.columns:
            logger.warning(f"   ‚ö†Ô∏è UUID column not found: {column}")
            continue
            
        logger.info(f"   üÜî Processing UUID column: {column}")
        
        # Handle null values based on strategy
        if null_handling == "skip":
            # Filter out null values
            before_count = result_df.count()
            result_df = result_df.filter(col(column).isNotNull())
            after_count = result_df.count()
            if before_count != after_count:
                logger.info(f"   üóëÔ∏è Removed {before_count - after_count:,} null UUID values")
                
        elif null_handling == "default":
            # Replace nulls with default UUID
            default_uuid = config.get("default_uuid", "00000000-0000-0000-0000-000000000000")
            result_df = result_df.fillna({column: default_uuid})
            logger.info(f"   üîÑ Replaced null UUIDs with default: {default_uuid}")
            
        elif null_handling == "error":
            # Check for nulls and error if found
            null_count = result_df.filter(col(column).isNull()).count()
            if null_count > 0:
                raise ValueError(f"Found {null_count} null values in UUID column {column}")
        
        # Cast to string to ensure proper UUID format
        result_df = result_df.withColumn(column, col(column).cast(StringType()))
    
    return result_df


def apply_aggregation(df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply data aggregation"""
    group_by = config.get("group_by", [])
    aggregations = config.get("aggregations", {})
    
    if not group_by or not aggregations:
        logger.warning("   ‚ö†Ô∏è Insufficient aggregation configuration")
        return df
    
    try:
        agg_expressions = []
        for column, agg_func in aggregations.items():
            if agg_func == "count":
                agg_expressions.append(count(column).alias(f"{column}_count"))
            elif agg_func == "sum":
                agg_expressions.append(sum(column).alias(f"{column}_sum"))
            elif agg_func == "avg":
                agg_expressions.append(avg(column).alias(f"{column}_avg"))
            elif agg_func == "max":
                agg_expressions.append(max(column).alias(f"{column}_max"))
            elif agg_func == "min":
                agg_expressions.append(min(column).alias(f"{column}_min"))
        
        if agg_expressions:
            result_df = df.groupBy(*group_by).agg(*agg_expressions)
            logger.info(f"   üìä Aggregated by {group_by} with {len(agg_expressions)} metrics")
            return result_df
        else:
            logger.warning("   ‚ö†Ô∏è No valid aggregation expressions")
            return df
            
    except Exception as e:
        logger.error(f"   ‚ùå Aggregation failed: {str(e)}")
        raise


def apply_custom_sql(spark: SparkSession, df: DataFrame, config: Dict[str, Any]) -> DataFrame:
    """Apply custom SQL transformation"""
    sql_query = config.get("sql", "")
    table_name = config.get("temp_table_name", "source_table")
    
    if not sql_query:
        logger.warning("   ‚ö†Ô∏è No SQL query specified")
        return df
    
    try:
        # Create temporary view
        df.createOrReplaceTempView(table_name)
        result_df = spark.sql(sql_query)
        logger.info(f"   üìù Applied custom SQL transformation")
        return result_df
    except Exception as e:
        logger.error(f"   ‚ùå Custom SQL failed: {str(e)}")
        raise


def perform_quality_checks(df: DataFrame, quality_rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Perform data quality validation"""
    if not quality_rules:
        return {"overall_score": 100, "checks": []}
    
    results = {"checks": [], "passed": 0, "total": len(quality_rules)}
    
    for rule in quality_rules:
        rule_name = rule.get("name", "Unnamed Rule")
        rule_type = rule.get("type", "")
        
        try:
            if rule_type == "not_null":
                columns = rule.get("columns", [])
                for column in columns:
                    if column in df.columns:
                        null_count = df.filter(col(column).isNull()).count()
                        passed = null_count == 0
                        results["checks"].append({
                            "name": f"{rule_name} - {column}",
                            "passed": passed,
                            "details": f"Null count: {null_count}"
                        })
                        if passed:
                            results["passed"] += 1
            
            elif rule_type == "unique":
                column = rule.get("column", "")
                if column in df.columns:
                    total_count = df.count()
                    distinct_count = df.select(column).distinct().count()
                    passed = total_count == distinct_count
                    results["checks"].append({
                        "name": f"{rule_name} - {column}",
                        "passed": passed,
                        "details": f"Total: {total_count}, Distinct: {distinct_count}"
                    })
                    if passed:
                        results["passed"] += 1
                        
        except Exception as e:
            logger.error(f"Quality check failed: {rule_name} - {str(e)}")
            results["checks"].append({
                "name": rule_name,
                "passed": False,
                "details": f"Error: {str(e)}"
            })
    
    # Calculate overall score
    if results["total"] > 0:
        results["overall_score"] = round((results["passed"] / results["total"]) * 100, 2)
    else:
        results["overall_score"] = 100
    
    return results


def log_quality_results(quality_results: Dict[str, Any]) -> None:
    """Log data quality check results"""
    score = quality_results.get("overall_score", 0)
    passed = quality_results.get("passed", 0)
    total = quality_results.get("total", 0)
    
    logger.info(f"üìä Data Quality Score: {score}% ({passed}/{total} checks passed)")
    
    for check in quality_results.get("checks", []):
        status = "‚úÖ" if check["passed"] else "‚ùå"
        logger.info(f"   {status} {check['name']}: {check['details']}")


def write_target_data(glueContext: GlueContext, df: DataFrame, target_config: Dict[str, Any]) -> None:
    """Write data to the configured target"""
    target_type = target_config.get("type")
    target_name = target_config.get("name", "Unknown")
    connection_details = target_config.get("connection_details", {})
    output_format = target_config.get("output_format", "parquet")
    
    logger.info(f"üì§ Writing to {target_type} target: {target_name} (format: {output_format})")
    
    try:
        if target_type == "S3":
            write_s3_target(glueContext, df, connection_details, output_format)
        elif target_type == "Database":
            write_database_target(glueContext, df, connection_details)
        elif target_type == "DataLake":
            write_datalake_target(glueContext, df, connection_details, output_format)
        else:
            raise ValueError(f"Unsupported target type: {target_type}")
            
    except Exception as e:
        logger.error(f"‚ùå Failed to write target data: {str(e)}")
        raise


def write_s3_target(glueContext: GlueContext, df: DataFrame, connection_details: Dict[str, Any], output_format: str) -> None:
    """Write data to S3"""
    bucket = connection_details.get("bucket", "")
    prefix = connection_details.get("prefix", "")
    partition_columns = connection_details.get("partition_by", [])
    
    s3_path = f"s3://{bucket}/{prefix}" if prefix else f"s3://{bucket}/"
    
    # Convert DataFrame to DynamicFrame
    dyf = DynamicFrame.fromDF(df, glueContext, "target_data")
    
    # Write options
    write_options = {
        "path": s3_path
    }
    
    if partition_columns:
        write_options["partitionKeys"] = partition_columns
        logger.info(f"   üìÅ Partitioning by: {partition_columns}")
    
    if output_format.lower() == "parquet":
        glueContext.write_dynamic_frame.from_options(
            frame=dyf,
            connection_type="s3",
            format="glueparquet",
            connection_options=write_options,
            format_options={"compression": "snappy"}
        )
    elif output_format.lower() == "json":
        glueContext.write_dynamic_frame.from_options(
            frame=dyf,
            connection_type="s3",
            format="json",
            connection_options=write_options
        )
    else:
        raise ValueError(f"Unsupported S3 output format: {output_format}")
    
    logger.info(f"‚úÖ Data written to: {s3_path}")


def write_database_target(glueContext: GlueContext, df: DataFrame, connection_details: Dict[str, Any]) -> None:
    """Write data to database"""
    connection_name = connection_details.get("connection_name")
    table_name = connection_details.get("table_name")
    write_mode = connection_details.get("write_mode", "append")
    
    # Convert DataFrame to DynamicFrame
    dyf = DynamicFrame.fromDF(df, glueContext, "target_data")
    
    if connection_name:
        # Use Glue Data Catalog connection
        glueContext.write_dynamic_frame.from_catalog(
            frame=dyf,
            database=connection_details.get("database"),
            table_name=table_name,
            connection_name=connection_name
        )
    else:
        # Direct JDBC connection
        connection_options = {
            "url": connection_details.get("jdbc_url"),
            "dbtable": table_name,
            "user": connection_details.get("username", ""),
            "password": connection_details.get("password", "")
        }
        
        glueContext.write_dynamic_frame.from_options(
            frame=dyf,
            connection_type="postgresql",
            connection_options=connection_options
        )
    
    logger.info(f"‚úÖ Data written to database table: {table_name}")


def write_datalake_target(glueContext: GlueContext, df: DataFrame, connection_details: Dict[str, Any], output_format: str) -> None:
    """Write data to data lake (Delta Lake, etc.)"""
    # For now, treat as S3 with special format handling
    logger.info("‚ÑπÔ∏è Writing to data lake (using S3 backend)")
    write_s3_target(glueContext, df, connection_details, output_format)


if __name__ == "__main__":
    main()